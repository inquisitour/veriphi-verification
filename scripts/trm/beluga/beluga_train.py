#!/usr/bin/env python3
"""
Train BelugaTRM on Airbus logistics constraint satisfaction problems
Optimizes jig assignments to satisfy flight, rack, and schedule constraints

Correct AMP scaler usage following PyTorch best practices
"""

import os
import sys
import time
import argparse
import torch
from torch import nn
from torch.utils.data import DataLoader

# Add src to path for imports
script_dir = os.path.dirname(os.path.abspath(__file__))
repo_root = os.path.dirname(os.path.dirname(script_dir))
if repo_root not in sys.path:
    sys.path.insert(0, repo_root)

from core.models.beluga_data_loader import create_beluga_dataloader, BelugaProblem
from core.models.beluga_trm_model import create_beluga_trm, get_model_dims_from_problem

DEVICE = torch.device(os.environ.get("VERIPHI_DEVICE", "cuda" if torch.cuda.is_available() else "cpu"))

def custom_collate(batch):
    """Custom collate function to handle BelugaProblem objects"""
    states = torch.cat([item[0] for item in batch], dim=0)
    problems = [item[1] for item in batch]
    return states, problems[0] if len(problems) == 1 else problems


def evaluate_model(model, dataloader, epoch, device=DEVICE):
    """
    Evaluate model on validation set
    
    Returns:
        avg_loss: average constraint violation loss
        avg_constraints: breakdown of constraint losses
    """
    model.eval()
    total_loss = 0.0
    total_flight_cap = 0.0
    total_rack_cap = 0.0
    total_schedule = 0.0
    total_balance = 0.0
    total_type_match = 0.0
    num_batches = 0
    
    with torch.no_grad():
        for state_batch, problem_batch in dataloader:
            # Currently batch_size=1, so problem_batch is list with 1 element
            problem = problem_batch[0] if isinstance(problem_batch, (list, tuple)) else problem_batch
            
            state_batch = state_batch.to(device)
            
            # Forward pass
            logits = model(state_batch)
            
            # Compute constraint loss
            loss = model.compute_constraint_loss(logits, problem)
            total_loss += loss.item()
            
            # Get constraint breakdown
            assignments = torch.argmax(logits.view(1, model.num_jigs, model.action_space), dim=-1)
            probs = torch.softmax(logits.view(1, model.num_jigs, model.action_space), dim=-1)
            
            # Create jig mask for individual constraint loss computation
            actual_num_jigs = problem.num_jigs
            jig_mask = torch.zeros(model.num_jigs, dtype=torch.bool, device=device)
            jig_mask[:actual_num_jigs] = True
            
            total_flight_cap += model._compute_flight_capacity_loss(probs[0], problem, jig_mask).item()
            total_rack_cap   += model._compute_rack_capacity_loss(probs[0], problem, jig_mask).item()
            total_schedule   += model._compute_schedule_constraint_loss(probs[0], problem, jig_mask).item()
            total_balance    += model._compute_flight_balance_loss(probs[0], problem, jig_mask).item()
            total_type_match += model._compute_type_matching_loss(probs[0], problem, jig_mask).item()
            
            num_batches += 1
    
    avg_loss = total_loss / max(num_batches, 1)
    avg_constraints = {
        'flight_capacity': total_flight_cap / max(num_batches, 1),
        'rack_capacity': total_rack_cap / max(num_batches, 1),
        'schedule': total_schedule / max(num_batches, 1),
        'balance': total_balance / max(num_batches, 1),
        'type_matching': total_type_match / max(num_batches, 1)
    }
    
    return avg_loss, avg_constraints


def main():
    parser = argparse.ArgumentParser(description='Train BelugaTRM on logistics problems')
    parser.add_argument('--data-dir', type=str, required=True, 
                       help='Path to Beluga dataset root (containing deterministic/training/)')
    parser.add_argument('--epochs', type=int, default=100, 
                       help='Number of epochs (default: 100)')
    parser.add_argument('--lr', type=float, default=0.001, 
                       help='Learning rate (default: 0.001)')
    parser.add_argument('--batch-size', type=int, default=1, 
                       help='Batch size (default: 1, constraint problems typically use 1)')
    parser.add_argument('--checkpoint-dir', type=str, default='checkpoints', 
                       help='Checkpoint save directory')
    parser.add_argument('--log-dir', type=str, default='logs', 
                       help='Log directory')
    parser.add_argument('--eval-every', type=int, default=5, 
                       help='Evaluate every N epochs')
    parser.add_argument('--num-workers', type=int, default=2, 
                       help='Number of data loading workers')
    args = parser.parse_args()
    
    epochs = args.epochs
    lr = args.lr
    
    # Create directories
    os.makedirs(args.checkpoint_dir, exist_ok=True)
    os.makedirs(args.log_dir, exist_ok=True)
    
    print(f"üöÄ Beluga TRM Training")
    print(f"=" * 60)
    print(f"Dataset: {args.data_dir}")
    print(f"Device: {DEVICE}")
    print(f"Epochs: {epochs} | LR: {lr} | Batch size: {args.batch_size}")
    print()
    
    # Load data
    print(f"üì¶ Loading training data...")
    train_split_dir = os.path.join(args.data_dir, "training")
    if not os.path.exists(train_split_dir):
        print(f"‚ùå Training directory not found: {train_split_dir}")
        return
    
    train_loader = create_beluga_dataloader(
        args.data_dir,
        split="training",
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0  # Changed from args.num_workers
    )

    # Override collate_fn
    train_loader.collate_fn = custom_collate
    train_loader.pin_memory = False  # Disable pin_memory for compatibility
    
    print(f"‚úÖ Loaded {len(train_loader.dataset)} training problems")
    
    # Get first problem to determine model dimensions
    print(f"\nüîç Analyzing problem structure...")
    first_state, first_problem = train_loader.dataset[0]
    model_dims = get_model_dims_from_problem(first_problem)
    
    print(f"\nüìä Model Configuration:")
    for key, value in model_dims.items():
        print(f"   {key}: {value}")
    
    # Create model
    print(f"\nüî® Creating BelugaTRM model...")
    model = create_beluga_trm(**model_dims)
    
    num_params = sum(p.numel() for p in model.parameters())
    print(f"   Total parameters: {num_params:,}")
    print(f"   Model on device: {next(model.parameters()).device}")
    
    # Optimizer and scheduler
    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs, eta_min=1e-6)

    checkpoint_path = os.path.join(args.checkpoint_dir, 'beluga_trm_best.pt')
    start_epoch = 1
    if os.path.exists(checkpoint_path):
        print(f"üìÇ Loading checkpoint from {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        opt.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        print(f"   Resuming from epoch {start_epoch}")
    
    # AMP disabled due to scaler initialization bug in PyTorch
    use_amp = False
    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)
    
    print(f"\nüî• Starting training...")
    print(f"   AMP enabled: {use_amp}")
    print(f"=" * 60)
    
    best_loss = float('inf')
    training_log = []
    
    for ep in range(start_epoch, epochs + 1):
        model.train()
        total_loss = 0.0
        total_flight_cap = 0.0
        total_rack_cap = 0.0
        total_schedule = 0.0
        total_balance = 0.0
        total_type_match = 0.0
        num_batches = 0
        t0 = time.time()
        
        for batch_idx, (state_batch, problem_batch) in enumerate(train_loader):
            # Get problem (batch_size=1 typically)
            problem = problem_batch if not isinstance(problem_batch, (list, tuple)) else problem_batch[0]
            
            state_batch = state_batch.to(DEVICE)
            
            # Training step
            opt.zero_grad(set_to_none=True)
            
            # Correct AMP pattern following PyTorch docs
            # https://pytorch.org/docs/stable/notes/amp_examples.html
            with torch.cuda.amp.autocast(enabled=use_amp):
                # Forward pass
                logits = model(state_batch)
                
                # Compute constraint loss
                loss = model.compute_constraint_loss(logits, problem)
            
            # ----- backward + clip + step (no AMP) -----
            loss.backward()

            # single clipping step
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)
            print(f"   grad_norm={float(grad_norm):.6f}")

            opt.step()
            # -------------------------------------------
            
            total_loss += loss.item()
            
            # Track constraint breakdown (without gradients)
            with torch.no_grad():
                assignments = torch.argmax(logits.view(1, model.num_jigs, model.action_space), dim=-1)
                probs = torch.softmax(logits.view(1, model.num_jigs, model.action_space), dim=-1)
                
                # Create jig mask for individual constraint loss computation
                actual_num_jigs = problem.num_jigs
                jig_mask = torch.zeros(model.num_jigs, dtype=torch.bool, device=DEVICE)
                jig_mask[:actual_num_jigs] = True
                
                total_flight_cap += model._compute_flight_capacity_loss(probs[0], problem, jig_mask).item()
                total_rack_cap   += model._compute_rack_capacity_loss(probs[0], problem, jig_mask).item()
                total_schedule   += model._compute_schedule_constraint_loss(probs[0], problem, jig_mask).item()
                total_balance    += model._compute_flight_balance_loss(probs[0], problem, jig_mask).item()
                total_type_match += model._compute_type_matching_loss(probs[0], problem, jig_mask).item()
            
            num_batches += 1
        
        # Step scheduler
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        epoch_time = time.time() - t0
        
        # Compute averages
        avg_loss = total_loss / max(num_batches, 1)
        avg_flight_cap = total_flight_cap / max(num_batches, 1)
        avg_rack_cap = total_rack_cap / max(num_batches, 1)
        avg_schedule = total_schedule / max(num_batches, 1)
        avg_balance = total_balance / max(num_batches, 1)
        avg_type_match = total_type_match / max(num_batches, 1)
        
        # Log
        log_entry = {
            'epoch': ep,
            'loss': avg_loss,
            'flight_capacity': avg_flight_cap,
            'rack_capacity': avg_rack_cap,
            'schedule': avg_schedule,
            'balance': avg_balance,
            'type_matching': avg_type_match,
            'lr': current_lr,
            'time': epoch_time
        }
        training_log.append(log_entry)
        
        # Print progress
        if ep % args.eval_every == 0 or ep == 1 or ep == epochs:
            print(f"Epoch {ep:3d}/{epochs}  loss={avg_loss:.4f}  lr={current_lr:.2e}  time={epoch_time:.1f}s")
            print(f"   Constraints: flight={avg_flight_cap:.3f} rack={avg_rack_cap:.3f} "
                  f"sched={avg_schedule:.3f} bal={avg_balance:.3f} type={avg_type_match:.3f}")
            
            # Save checkpoint if best
            if avg_loss < best_loss:
                best_loss = avg_loss
                save_path = os.path.join(args.checkpoint_dir, 'beluga_trm_best.pt')
                torch.save({
                    'epoch': ep,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': opt.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'loss': avg_loss,
                    'model_dims': model_dims,
                    'args': vars(args)
                }, save_path)
                print(f"   üíæ Saved best checkpoint (loss: {avg_loss:.4f})")
        else:
            # Quick progress update
            print(f"Epoch {ep:3d}/{epochs}  loss={avg_loss:.4f}  lr={current_lr:.2e}  time={epoch_time:.1f}s")
    
    # Save final checkpoint
    final_save_path = os.path.join(args.checkpoint_dir, 'beluga_trm_final.pt')
    torch.save({
        'epoch': epochs,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': opt.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'loss': avg_loss,
        'model_dims': model_dims,
        'args': vars(args)
    }, final_save_path)
    
    print(f"\n‚úÖ Training complete!")
    print(f"   Best loss: {best_loss:.4f}")
    print(f"   Final loss: {avg_loss:.4f}")
    print(f"   Best checkpoint: {os.path.join(args.checkpoint_dir, 'beluga_trm_best.pt')}")
    print(f"   Final checkpoint: {final_save_path}")
    
    # Save training log
    import csv
    log_path = os.path.join(args.log_dir, f'beluga_training_log_{int(time.time())}.csv')
    with open(log_path, 'w', newline='') as f:
        if training_log:
            writer = csv.DictWriter(f, fieldnames=training_log[0].keys())
            writer.writeheader()
            writer.writerows(training_log)
    print(f"   Training log: {log_path}")
    
    print(f"\nüéØ Next Steps:")
    print(f"   1. Evaluate on validation set")
    print(f"   2. Visualize constraint satisfaction over epochs")
    print(f"   3. Test on held-out problems")


if __name__ == "__main__":
    main()