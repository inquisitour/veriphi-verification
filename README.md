# ğŸ§  Veriphi: Neural Network Robustness Verification

A **GPUâ€‘accelerated verification stack** combining **attackâ€‘guided adversarial search** with **formal bound certification**  
(Î±â€‘, Î²â€‘CROWN via [autoâ€‘LiRPA](https://github.com/Verified-Intelligence/auto_LiRPA)).

It answers a simple but critical question:

> **"Is this model provably robust within Îµ under Lâˆ or L2 perturbations?"**

â€¦and returns **verified / falsified**, with measured **runtime & memory**.

---

## ğŸš€ New Highlights

âœ… **Attackâ€‘Guided Verification:**  
   Fast falsification via FGSM + Iâ€‘FGSM, then formal verification using Î±â€‘, Î²â€‘CROWN.

âœ… **TRMâ€‘MLP Integration:**  
   Support for **Tiny Recursive Models (TRM)** â€” verified using the same unified pipeline.

âœ… **GPUâ€‘Accelerated Verification:**  
   Works seamlessly on **A100, RTX** or any CUDAâ€‘enabled GPU.

âœ… **Cross-Dataset Validation:**  
   Comprehensive verification on **MNIST** and **CIFAR-10** with 3 training methods (Baseline, IBP, PGD).

âœ… **Multi-Bound Comparison:**  
   Systematic evaluation of **CROWN, Î±-CROWN, Î²-CROWN** across datasets and models.

---

## ğŸ”§ Install (reproducible)

```bash
# Clone
git clone https://github.com/inquisitour/veriphi-verification.git
cd veriphi-verification

# Python env
python3 -m venv venv
source venv/bin/activate

# Installation
pip install git+https://github.com/Verified-Intelligence/auto_LiRPA.git
pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu121
```

Verify your toolchain:
```bash
python verify_installation.py
```

---

## ğŸš€ Quick smoke test

```bash
# From repo root
source venv/bin/activate
export PYTHONPATH="$PWD/src:$PYTHONPATH"
python scripts/core_smoke.py
```

---

## âš¡ GPU mode

Veriphi fully supports **CUDA (A100, RTX, etc.)**.

```bash
# Enable GPU device
export VERIPHI_DEVICE=cuda
```

All engines, attacks, and models will automatically run on the GPU.

---

## ğŸ“Š Complete Verification Results

### MNIST (28Ã—28 grayscale, 512 samples)

| Training Method | Îµ=0.04 | Îµ=0.06 | Îµ=0.08 | Îµ=0.1 | **Winner** |
|----------------|--------|--------|--------|-------|------------|
| **IBP (1/255)** | 47% | **77%** | **78%** | **75%** | ğŸ¥‡ |
| **PGD (2/255)** | 43% | 63% | 65% | 60% | ğŸ¥ˆ |
| **Baseline** | 0% | 0% | 0% | 0% | âŒ |

### CIFAR-10 (32Ã—32 RGB, 512 samples)

| Training Method | Îµ=0.001 | Îµ=0.002 | Îµ=0.004 | Îµ=0.006 | **Winner** |
|----------------|---------|---------|---------|---------|------------|
| **PGD (8/255)** | **94%** | **90%** | **80%** | **67%** | ğŸ¥‡ |
| **IBP (2/255)** | 78% | 51% | 10% | 1% | ğŸ¥ˆ |
| **Baseline** | 82% | 55% | 13% | 1% | ğŸ¥‰ |

**Key Finding:** Training method effectiveness depends on dataset complexity:
- **IBP dominates on simple MNIST** (75-78% @ Îµ=0.06-0.1)
- **PGD dominates on complex CIFAR-10** (48-95% across all Îµ)
- **Bound methods (Î±/Î²-CROWN)** provide <5% improvement over CROWN

---

## ğŸ§© TRM Experiments

### Training Scripts

```bash
# MNIST
python scripts/trm/core/trm_tiny_train.py              # Baseline
python scripts/trm/core/trm_tiny_advtrain.py           # PGD adversarial
python scripts/trm/core/trm_ibp_train.py               # IBP certified

# CIFAR-10
python scripts/trm/core/trm_tiny_train_cifar10.py      # Baseline
python scripts/trm/core/trm_tiny_advtrain_cifar10.py   # PGD adversarial
python scripts/trm/core/trm_ibp_train_cifar10.py       # IBP certified
```

### Verification Sweeps

```bash
# MNIST - Full sweep (512 samples, 3 bounds)
python scripts/trm/core/trm_tiny_sweep.py \
  --samples 512 \
  --eps 0.01,0.02,0.03,0.04,0.06,0.08,0.1 \
  --bound CROWN

# CIFAR-10 - Full sweep
python scripts/trm/core/trm_tiny_sweep_cifar10.py \
  --samples 512 \
  --eps 0.001,0.002,0.004,0.006,0.008,0.01 \
  --bound CROWN
```

### Generate Reports

```bash
# MNIST comprehensive report
python scripts/trm/reports/trm_full_visual_report_mnist.py

# CIFAR-10 comprehensive report
python scripts/trm/reports/trm_full_visual_report_cifar10.py

# Bound comparison across datasets
python scripts/trm/reports/trm_compare_bounds_report.py
```

**Generated outputs:**
- `reports/trm_mnist_full_report_*.pdf` - MNIST analysis
- `reports/trm_cifar10_full_report_*.pdf` - CIFAR-10 analysis
- `reports/trm_compare_bounds_report_*.pdf` - Cross-bound comparison
- `plots/` - Individual visualization files

---

## ğŸ—ï¸ Architecture Overview

```
scripts/trm/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ trm_tiny_train*.py           # Training scripts (MNIST/CIFAR-10)
â”‚   â”œâ”€â”€ trm_tiny_advtrain*.py        # PGD adversarial training
â”‚   â”œâ”€â”€ trm_ibp_train*.py            # IBP certified training
â”‚   â”œâ”€â”€ trm_tiny_verify*.py          # Single-sample verification
â”‚   â””â”€â”€ trm_tiny_sweep*.py           # Batch verification sweeps
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ trm_visualize_results.py           # Generate plots
â”‚   â”œâ”€â”€ trm_full_visual_report_*.py        # PDF reports (MNIST/CIFAR-10)
â”‚   â””â”€â”€ trm_compare_bounds_report.py       # Multi-bound comparison
â””â”€â”€ presentation/
    â””â”€â”€ trm_presentation_slide.py          # PowerPoint generation

checkpoints/          # Trained model weights
logs/                 # CSV verification results
plots/                # Generated visualizations
reports/              # PDF reports
```

---

## ğŸ“Š Performance Metrics

**Verification Efficiency:**
- **MNIST:** ~0.15-0.24s per sample
- **CIFAR-10:** ~0.09-0.24s per sample
- **GPU Memory:** 18-53 MB per sample (A100)

**Bound Method Comparison:**
- **CROWN:** Fastest, baseline accuracy
- **Î±-CROWN:** +0-5% accuracy, ~1.2Ã— slower
- **Î²-CROWN:** +0-9% accuracy (baselines only), ~1.5Ã— slower

---

## ğŸ§­ Roadmap

| Stage | Goal | Status |
|--------|------|--------|
| 1ï¸âƒ£ | CUDA acceleration (A100 verified) | âœ… |
| 2ï¸âƒ£ | TRM-MLP recursive architecture | âœ… |
| 3ï¸âƒ£ | Multiple training methods (Baseline, IBP, PGD) | âœ… |
| 4ï¸âƒ£ | Cross-dataset validation (MNIST + CIFAR-10) | âœ… |
| 5ï¸âƒ£ | Multi-bound comparison (CROWN, Î±/Î²-CROWN) | âœ… |
| 6ï¸âƒ£ | Comprehensive reporting & visualization | âœ… |
| 7ï¸âƒ£ | Scale to larger models & datasets | ğŸ”œ |

---

## ğŸ“’ Guides

- [VSC5 Connection Guide (CLI)](./docs/vsc5_connection_readme.md)
- [Benchmarking Guide](./docs/trm_scaling_readme.md)

---

## ğŸ“š References

- **autoâ€‘LiRPA Docs:** https://auto-lirpa.readthedocs.io/  
- **Î±,Î²â€‘CROWN Repo:** https://github.com/Verified-Intelligence/alpha-beta-CROWN  
- **Tiny Recursive Models:** https://github.com/SamsungSAILMontreal/TinyRecursiveModels  
- **VNNâ€‘COMP:** https://sites.google.com/view/vnn2024  

---

## ğŸ“„ License

MIT â€” see `LICENSE`.

---
 
"*Bridging adversarial testing and formal verification for truly robust neural networks.*"
